{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chap 13 Naive Bayes\n",
    "\n",
    "#### A Really Dumb Spam Filter\n",
    "\n",
    "Let `S` be the event \"the message is spam\" and `V` be the event \"the message contains the word *viagra*.\". \n",
    "`P(S/V) = P(message is spam and contains viagra/ probability that message contains viagra)`\n",
    "\n",
    "If we have a large collection of messages we know are spam, and a large collection we know are not spam, we can estimate `P(V|S)` and `P(V|¬S)`. If we further assume that any message is equally likely to be spam or not-spam (so that `P(S) = P(¬S) = 0.5`), then:\n",
    "\n",
    "`P(S|V) = P(V|S) / P(V|S) + P(V|¬S)`\n",
    "\n",
    "E.g if 50% of spam messages have the word *viagra*, but only 1% of nonspam messages do, then the probability that any given *viagra*-containing email is spam is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803921568627451"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5/(0.5 +0.01)) # 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implimentation\n",
    "\n",
    "Building the classifier. First, we will make a function to tokenize messages into distinct words. We'll first convert each message to lowercase; use `re.findall()` to extract \"words\" consisting of letters, numbers and apostrophes; finally use `set()` to get just the distinct words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower() # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9]+\", message) # extract the words\n",
    "    return set(all_words) # remove duplicates   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next function will count the words in a labeled training set of messages. We'll have to return a dictionary whose keys are words, and whose values are two-element lists[spam_count, non_spam_count] corresponding to how many times we saw that word in both spam and nonspam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda:[0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
