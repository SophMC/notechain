{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial #1 : Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "path=\"/home/sophie/projects/kaggleBOW/\"\n",
    "\n",
    "# quoting = 3 tells python to ignore doubled quotes\n",
    "train = pd.read_csv(\"%slabeledTrainData.tsv\"%path, header = 0, delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Text Preprocessing\n",
    "\n",
    "Remove HTML Markup with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Initialize BeautifulSoup object on a single movei review\n",
    "example1 = BeautifulSoup(train[\"review\"][0])\n",
    "\n",
    "# get_text gives you the etxt of the review, with no tags or markup.\n",
    "# not considered reliable practise remove markup with regular expressions to do this however\n",
    "print(example1.get_text()[0:100]) # first 100 characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `re` package to remove punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With all this stuff going down at the moment with MJ i ve started listening to his music  watching \n"
     ]
    }
   ],
   "source": [
    "# ^ means \"not\"\n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", example1.get_text())\n",
    "print(letters_only[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert reviews to lower case and split them into individual words (\"tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_case = letters_only.lower() \n",
    "words = lower_case.split() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLTK library to remove all of the stop words (\"a\",\"is\",\"and\") etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download() # download text data sets, including stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # import the stop word list\n",
    "print(stopwords.words(\"english\")[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print(words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more we could go here. NLTK could allow us to do \"stemming\" (Porter Stemming) which allows us to treat \"message\", \"messaging\" and \"messages\" all the same.\n",
    "\n",
    "### Pulling it all together\n",
    "\n",
    "Now to make some reusable code to clean all 25 000 training reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    \"\"\"Convert a raw review into a string of words.\n",
    "    The input it a single string(raw movie review)\n",
    "    and the output is a single string (a preprocessed\n",
    "    movie review)\"\"\"\n",
    "    # remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    # convert to lower case\n",
    "    words = letters_only.lower().split()\n",
    "                          \n",
    "    # convert stopwords into a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "                          \n",
    "    # remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Join the words back into one string, seperated by space\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# try a call on a single review\n",
    "print(review_to_words(train[\"review\"][0])[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will loop through and clean all of the training set at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length of the movie review list\n",
    "print (\"Cleaning and parsing the training set movie reviews...\")\n",
    "clean_train_reviews = []\n",
    "for i in range(0, num_reviews):\n",
    "    \n",
    "    # if the index is evenly divisible by 1000, print a message\n",
    "    if ((i+1)% 5000 == 0):\n",
    "        print (\"Review %d of %d\\n\" %(i+1, num_reviews))\n",
    "    \n",
    "    # Call our function for each one, and add the result to the list of clean reviews\n",
    "    clean_train_reviews.append(review_to_words(train[\"review\"][i]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features from a Bag of Words (Using scikit-learn)\n",
    "\n",
    "Bag of words takes all words from all reviews (documents) to create a *vocabulary*.   \n",
    "Models each review by counting the number of times each word appears.\n",
    "\n",
    "We will choose a maximum vocabulary size of 5000 of the most frequent words.    \n",
    "we will use `feature_extraction` from scikit-learn to create bag-of-words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words....\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating the bag of words....\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform()` does two functions: First, it fits the model and learns the vocabulary; second, it transforms our training data into feature vectors. The input to `fit_transform` should be a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "125000000\n",
      "1974964\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)\n",
    "print(train_data_features.size)\n",
    "print(len(train_data_features[train_data_features != 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'absence', 'absent', 'absolute', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the counts of each word in the vocabulary\n",
    "#counts = Counter(word += 1 for word in vocab)\n",
    "\n",
    "counts = Counter()\n",
    "#for w in train_data_features2:\n",
    "#    counts[w] += 1\n",
    "\n",
    "#[counts[w] for w in list(counts) if counts[w] > 1]\n",
    "#print(train_data_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n",
      "[ 187  125  108  454 1259   85  116   83  352 1485  306  192   91   98  297\n",
      "  485  203  300  130  144   92  318  200   88  124  296  186   81  284  123\n",
      "  179  139  124   90  971 1251  658 6490 3354  311   83 2389 4486 1219  369\n",
      "  394  793 4237  148  302   98  453   80  154  810  439  166  347  337  113\n",
      "  124  621  134  101  510  376  100   90  153  510  204   91  259   90  346\n",
      "   93  113  104  126  343  212  255  187  128 1121  233  361   94  249  111\n",
      " 1033  572   88   95  119  396  106   96   81  120]\n"
     ]
    }
   ],
   "source": [
    "# sum up the counts of each vocabulary word. axis = 0 referes to rows.\n",
    "dist = np.sum(train_data_features, axis = 0)\n",
    "\n",
    "print(dist.shape)\n",
    "print(dist[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6490 acting\n",
      "9155 also\n",
      "9301 bad\n",
      "6414 best\n",
      "7022 character\n",
      "7154 characters\n",
      "7921 could\n",
      "12646 even\n",
      "40146 film\n",
      "6887 films\n",
      "9061 first\n",
      "9310 get\n",
      "15140 good\n",
      "9058 great\n",
      "6166 know\n",
      "6628 life\n",
      "20274 like\n",
      "6435 little\n",
      "6453 love\n",
      "8362 made\n",
      "8021 make\n",
      "6675 many\n",
      "44030 movie\n",
      "7663 movies\n",
      "9765 much\n",
      "6484 never\n",
      "26788 one\n",
      "9285 people\n",
      "6585 plot\n",
      "11736 really\n",
      "11474 see\n",
      "6679 seen\n",
      "6294 show\n",
      "11983 story\n",
      "7296 think\n",
      "12723 time\n",
      "6906 two\n",
      "6972 watch\n",
      "8026 way\n",
      "10661 well\n",
      "12436 would\n"
     ]
    }
   ],
   "source": [
    "# For each, print the vocabulary word and the number of times it appear in the training set\n",
    "# vocab and dist are now two nice lists we can zip together\n",
    "for tag, count in zip(vocab, dist):\n",
    "    if count > 6000:  # just look at the most popular words.\n",
    "        print (count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "As we have the original sentiment labels for each feature vector, we can do some supervised learning.   \n",
    "We'll use the Random Forest Classifier set with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as features and the sentiment labels \n",
    "# as the response variable\n",
    "\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Submission\n",
    "\n",
    "All that is left is to run the Random Forest on the test set and create a submission file. \n",
    "Note that this is the test data so we run \"transform\" instead of \"fit_transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the test data\n",
    "test = pd.read_csv(\"%stestData.tsv\"%path, header = 0, delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Verify that there are 25000 rows and 2 columns\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list and append the clean review one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0, num_reviews):\n",
    "    if ((i+1) % 5000 == 0):\n",
    "        print(\"Review %d of %d\\n\" %(i+1, num_reviews))\n",
    "    clean_review = review_to_words(test[\"review\"][i])\n",
    "    clean_test_reviews.append(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words out for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy the results to a pandas dataframe with an \"id\" column and a \"sentiment\" column\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\":result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use pandas to write the comma-separated output file\n",
    "path = \"/home/sophie/projects/kaggleBOW/submission/\"\n",
    "output.to_csv(\"%sBag_of_Words_model.csv\"%path, index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different things to try:\n",
    "\n",
    "- clean reviews differently\n",
    "- choose a different number of vocabulary words\n",
    "- Porter Stemming\n",
    "- Different classifier.\n",
    "\n",
    "Try it out on a different data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out the textmining module (just 3 reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textmining\n",
    "\n",
    "# This module allows us to skip the review_to_words step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some very short sample documents from the raw data. \n",
    "doc1 = train[\"review\"][0]\n",
    "doc2 = train[\"review\"][1]\n",
    "doc3 = train[\"review\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize class to create term-document matrix\n",
    "tdm = textmining.TermDocumentMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the documents\n",
    "tdm.add_doc(doc1)\n",
    "tdm.add_doc(doc2)\n",
    "tdm.add_doc(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can write out the term-document matrix to a csv : \n",
    "\n",
    "`tdm.write_csv('matrix.csv', cutoff=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'addition', 'bestest', 'performing', 'ridiculous', 'making', 'more', 'tourists', 'below', 'one']\n",
      "[1, 0, 1, 1, 0, 1, 0, 0, 0, 4]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 1, 0, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# But instead lets have a look at the rows directly\n",
    "for row in tdm.rows(cutoff=1):\n",
    "    print (row[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quite clearly shows how all of the words in all the documents are represented in each of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if we get the same results as above in regards. \n",
    "\n",
    "## Testing textmining with all the reviews to see how it compares with above.\n",
    "\n",
    "This time we need to create a loop to add all the documents to the term-document matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize class to create term-document matrix\n",
    "tdm = textmining.TermDocumentMatrix()\n",
    "\n",
    "for i in range(0, num_reviews):\n",
    "    \n",
    "    # if the index is evenly divisible by 1000, print a message\n",
    "    if ((i+1)% 5000 == 0):\n",
    "        print (\"Review %d of %d\\n\" %(i+1, num_reviews))\n",
    "    \n",
    "    # Call our function for each one, and add the result to the list of clean reviews\n",
    "    tdm.add_doc(train[\"review\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_counts = Counter()\n",
    "vocab_matrix_list = []\n",
    "# Lets look at the words in the vocab. The vocab is all the words, not just the top 5000.\n",
    "# This doesn't work.\n",
    "#for row in tdm.rows(cutoff=1):\n",
    "    #vocab_matrix_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salvages', 'gestures', 'turntable', 'purse', 'forefather', 'pota', 'rajiv', 'garnell', 'traipsing', 'yamaha']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# We miss a step in textmining - we don't have all of the clean documents to search through to get the counts this way.\n",
    "# We only have the csv matrix.\n",
    "\n",
    "print(vocab2[0:10])\n",
    "vocab_counts = Counter()\n",
    "\n",
    "for word in vocab2:\n",
    "    vocab_counts[word] += 1\n",
    "\n",
    "print(list(vocab_counts.values())[0:10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store the 5000 most common words in a new variable\n",
    "vocab_5000 = vocab_counts.most_common(5000)\n",
    "\n",
    "#print(sorted(list(vocab_5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
