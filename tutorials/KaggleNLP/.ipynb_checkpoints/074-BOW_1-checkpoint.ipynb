{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial #1 : Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "path=\"/home/sophie/projects/kaggleBOW/\"\n",
    "\n",
    "# quoting = 3 tells python to ignore doubled quotes\n",
    "train = pd.read_csv(\"%slabeledTrainData.tsv\"%path, header = 0, delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Text Preprocessing\n",
    "\n",
    "Remove HTML Markup with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Initialize BeautifulSoup object on a single movei review\n",
    "example1 = BeautifulSoup(train[\"review\"][0])\n",
    "\n",
    "# get_text gives you the etxt of the review, with no tags or markup.\n",
    "# not considered reliable practise remove markup with regular expressions to do this however\n",
    "print(example1.get_text()[0:100]) # first 100 characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `re` package to remove punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With all this stuff going down at the moment with MJ i ve started listening to his music  watching \n"
     ]
    }
   ],
   "source": [
    "# ^ means \"not\"\n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", example1.get_text())\n",
    "print(letters_only[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert reviews to lower case and split them into individual words (\"tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_case = letters_only.lower() \n",
    "words = lower_case.split() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLTK library to remove all of the stop words (\"a\",\"is\",\"and\") etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download() # download text data sets, including stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # import the stop word list\n",
    "print(stopwords.words(\"english\")[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print(words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more we could go here. NLTK could allow us to do \"stemming\" (Porter Stemming) which allows us to treat \"message\", \"messaging\" and \"messages\" all the same.\n",
    "\n",
    "### Pulling it all together\n",
    "\n",
    "Now to make some reusable code to clean all 25 000 training reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    \"\"\"Convert a raw review into a string of words.\n",
    "    The input it a single string(raw movie review)\n",
    "    and the output is a single string (a preprocessed\n",
    "    movie review)\"\"\"\n",
    "    # remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    # convert to lower case\n",
    "    words = letters_only.lower().split()\n",
    "                          \n",
    "    # convert stopwords into a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "                          \n",
    "    # remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Join the words back into one string, seperated by space\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# try a call on a single review\n",
    "print(review_to_words(train[\"review\"][0])[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will loop through and clean all of the training set at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length of the movie review list\n",
    "print (\"Cleaning and parsing the training set movie reviews...\")\n",
    "clean_train_reviews = []\n",
    "for i in range(0, num_reviews):\n",
    "    \n",
    "    # if the index is evenly divisible by 1000, print a message\n",
    "    if ((i+1)% 5000 == 0):\n",
    "        print (\"Review %d of %d\\n\" %(i+1, num_reviews))\n",
    "    \n",
    "    # Call our function for each one, and add the result to the list of clean reviews\n",
    "    clean_train_reviews.append(review_to_words(train[\"review\"][i]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features from a Bag of Words (Using scikit-learn)\n",
    "\n",
    "Bag of words takes all words from all reviews (documents) to create a *vocabulary*.   \n",
    "Models each review by counting the number of times each word appears.\n",
    "\n",
    "We will choose a maximum vocabulary size of 5000 of the most frequent words.    \n",
    "we will use `feature_extraction` from scikit-learn to create bag-of-words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words....\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating the bag of words....\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform()` does two functions: First, it fits the model and learns the vocabulary; second, it transforms our training data into feature vectors. The input to `fit_transform` should be a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'absence', 'absent', 'absolute', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the counts of each word in the vocabulary\n",
    "#counts = Counter(word += 1 for word in vocab)\n",
    "\n",
    "counts = Counter()\n",
    "#for w in train_data_features2:\n",
    "#    counts[w] += 1\n",
    "\n",
    "#[counts[w] for w in list(counts) if counts[w] > 1]\n",
    "#print(train_data_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n",
      "[ 187  125  108  454 1259   85  116   83  352 1485  306  192   91   98  297\n",
      "  485  203  300  130  144   92  318  200   88  124  296  186   81  284  123\n",
      "  179  139  124   90  971 1251  658 6490 3354  311   83 2389 4486 1219  369\n",
      "  394  793 4237  148  302   98  453   80  154  810  439  166  347  337  113\n",
      "  124  621  134  101  510  376  100   90  153  510  204   91  259   90  346\n",
      "   93  113  104  126  343  212  255  187  128 1121  233  361   94  249  111\n",
      " 1033  572   88   95  119  396  106   96   81  120]\n"
     ]
    }
   ],
   "source": [
    "# sum up the counts of each vocabulary word. axis = 0 referes to rows.\n",
    "dist = np.sum(train_data_features, axis = 0)\n",
    "\n",
    "print(dist.shape)\n",
    "print(dist[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6490 acting\n",
      "9155 also\n",
      "9301 bad\n",
      "6414 best\n",
      "7022 character\n",
      "7154 characters\n",
      "7921 could\n",
      "12646 even\n",
      "40146 film\n",
      "6887 films\n",
      "9061 first\n",
      "9310 get\n",
      "15140 good\n",
      "9058 great\n",
      "6166 know\n",
      "6628 life\n",
      "20274 like\n",
      "6435 little\n",
      "6453 love\n",
      "8362 made\n",
      "8021 make\n",
      "6675 many\n",
      "44030 movie\n",
      "7663 movies\n",
      "9765 much\n",
      "6484 never\n",
      "26788 one\n",
      "9285 people\n",
      "6585 plot\n",
      "11736 really\n",
      "11474 see\n",
      "6679 seen\n",
      "6294 show\n",
      "11983 story\n",
      "7296 think\n",
      "12723 time\n",
      "6906 two\n",
      "6972 watch\n",
      "8026 way\n",
      "10661 well\n",
      "12436 would\n"
     ]
    }
   ],
   "source": [
    "# For each, print the vocabulary word and the number of times it appear in the training set\n",
    "# vocab and dist are now two nice lists we can zip together\n",
    "for tag, count in zip(vocab, dist):\n",
    "    if count > 6000:  # just look at the most popular words.\n",
    "        print (count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
