{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "samtrain = pd.read_csv('/home/sophie/projects/LearnDataScience-master/datasets/samsung/samtrain.csv')\n",
    "samval = pd.read_csv('/home/sophie/projects/LearnDataScience-master/datasets/samsung/samval.csv')\n",
    "samtest=pd.read_csv('/home/sophie/projects/LearnDataScience-master/datasets/samsung/samtest.csv')\n",
    "\n",
    "# Use the RandomForests package from the scikits.learn collection of algorithms\n",
    "# Package name is sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "# For the we need ot convert the target column ('activity') to integer values\n",
    "# because RandomForest requires that.\n",
    "\n",
    "# We map activity to an integer according to :\n",
    "# laying = 1, sitting = 2, standing = 3, walk = 4, walkup = 5, walkdown = 6\n",
    "# Code is in supporting library randomforest.py\n",
    "\n",
    "# These names are converted to numbers via randomforest.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "Name: Unnamed: 0, dtype: int64\n",
      "0   -0.959434\n",
      "1   -0.979289\n",
      "Name: tAccMean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print samtrain.iloc[0:2,0] # what is that first column?\n",
    "\n",
    "# Remove the spare column ('unnamed' - howd it get there?) from vals and test\n",
    "samtrain.drop(samtrain.columns[0], axis=1, inplace=True)\n",
    "samval.drop(samval.columns[0], axis=1, inplace=True)\n",
    "samtest.drop(samtest.columns[0], axis=1, inplace=True)\n",
    "\n",
    "print samtrain.iloc[0:2,0] # now it is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sophie/projects/Samsung/bin\n"
     ]
    }
   ],
   "source": [
    "cd /home/sophie/projects/Samsung/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing the supporting randomforests.py\n",
    "import randomforests as rf\n",
    "\n",
    "samtrain = rf.remap_col(samtrain, 'activity')\n",
    "samval = rf.remap_col(samval,'activity')\n",
    "samtest = rf.remap_col(samtest,'activity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1315, 37)\n",
      "(1902, 37)\n",
      "(1485, 37)\n",
      "0    3\n",
      "1    3\n",
      "2    3\n",
      "3    3\n",
      "4    3\n",
      "Name: activity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# These are different lengths because they have a different number of volunteers data in them\n",
    "print samtrain.shape\n",
    "print samval.shape\n",
    "print samtest.shape\n",
    "\n",
    "# Now 'activity is in int form'\n",
    "print samtrain['activity'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble as sk\n",
    "\n",
    "rfc = sk.RandomForestClassifier(n_estimators = 500, oob_score=True) # Using 500 trees. This is the base model?\n",
    "\n",
    "train_data = samtrain[samtrain.columns[0:-2]] # dropping activity and subject columns\n",
    "train_truth = samtrain['activity']            # putting activity into a new var\n",
    "\n",
    "# Important to realise that .fit() is a METHOD being applied to rfc. rfc=model (as far as I can tell)\n",
    "model = rfc.fit(train_data, train_truth)      # fit(X, y[, sample_weight]) \tBuild a forest from the training set (X, y).\n",
    "                                              # X = training input samples, y = classification labels (target values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the difference between model and rfc? Nothing. See the next two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "            oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "            oob_score=True, random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# These are exactly the same\n",
    "print model\n",
    "print rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have seperated the data into input samples and classification labels(target values for the trees)\n",
    "We fit them together into model using rfc.fit() to build our forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97870722433460078"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at the oob (out of bag) score to see an estimate of the accuracy of our model\n",
    "rfc.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97870722433460078"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04032681  0.05118829  0.04156427  0.04440338  0.02406173  0.01086383\n",
      "  0.03055529  0.01689195  0.0515442   0.05658437  0.01735149  0.00851179\n",
      "  0.00824265  0.04819139  0.03443411  0.00602119  0.00429613  0.00407388\n",
      "  0.01127409  0.00787828  0.03229847  0.00561677  0.00529572  0.01536894\n",
      "  0.01823302  0.00853026  0.00915033  0.00890646  0.00438409  0.00400397\n",
      "  0.01041084  0.00705159  0.13070362  0.17654349  0.04524329]\n",
      "[ 0.04032681  0.05118829  0.04156427  0.04440338  0.02406173  0.01086383\n",
      "  0.03055529  0.01689195  0.0515442   0.05658437  0.01735149  0.00851179\n",
      "  0.00824265  0.04819139  0.03443411  0.00602119  0.00429613  0.00407388\n",
      "  0.01127409  0.00787828  0.03229847  0.00561677  0.00529572  0.01536894\n",
      "  0.01823302  0.00853026  0.00915033  0.00890646  0.00438409  0.00400397\n",
      "  0.01041084  0.00705159  0.13070362  0.17654349  0.04524329]\n",
      "35\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance. What are the top 10 important features?\n",
    "\n",
    "fi = enumerate(rfc.feature_importances_)  # give a value of importance to each feature\n",
    "cols = samtrain.columns                   # create a similar array just of array names\n",
    "[(value,cols[i]) for (i, value) in fi if value > 0.04]\n",
    "\n",
    "print rfc.feature_importances_ # this tells us the importance of each feature\n",
    "print model.feature_importances_ \n",
    "\n",
    "print len(rfc.feature_importances_)  # How is rfc linked to model?\n",
    "print len(cols)   # Note sure why we use activity and subject here too. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same procedure as from the training data\n",
    "val_data = samval[samval.columns[0:-2]]  # leave out last two columns, subject and activity\n",
    "val_truth = samval['activity']\n",
    "\n",
    "# This time we use predict() and val_truth is not needed. why?\n",
    "val_pred = rfc.predict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = samtest[samtest.columns[0:-2]]\n",
    "test_truth = samtest['activity']\n",
    "test_pred = rfc.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prediction Errors and Computer Error Measures **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy score for validation set = 0.832282 \n",
      "mean accuracy score for test set = 0.891582\n"
     ]
    }
   ],
   "source": [
    "print (\"mean accuracy score for validation set = %f \" % (rfc.score(val_data, val_truth)))\n",
    "print (\"mean accuracy score for test set = %f\" % (rfc.score(test_data, test_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the confusion matrix to see how observations were misclassified as other activities\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "test_cm = skm.confusion_matrix(test_truth, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAAEWCAYAAAC64wHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc2046f890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "pl.matshow(test_cm)\n",
    "pl.title('Confusion matrix for test data\\n'\n",
    "        + '                               ')\n",
    "pl.colorbar\n",
    "pl.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute some other measures of prediction goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.891582\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print (\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.894418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "print (\"Precision = %f\" %(skm.precision_score(test_truth,test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.891582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "print (\"Recall = %f\" %(skm.recall_score(test_truth,test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.892030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "# F1 Score\n",
    "print (\"F1 score = %f\" %(skm.f1_score(test_truth,test_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
